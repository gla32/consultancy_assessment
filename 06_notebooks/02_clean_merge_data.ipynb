{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Merging Script for Consultancy Assessment\n",
    "\n",
    "This notebook cleans and merges the three datasets into a single analysis-ready file:\n",
    "1. UNICEF health indicators (ANC4 and SBA)\n",
    "2. UN Population data (2022 birth projections)\n",
    "3. Under-5 mortality reduction status\n",
    "\n",
    "**Author:** Data Analyst  \n",
    "**Date:** 2025-01-27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# File paths - handle different working directories\n",
    "import os\n",
    "\n",
    "# Get the project root directory (go up from current script location)\n",
    "current_dir = Path.cwd()\n",
    "if 'notebooks' in str(current_dir):\n",
    "    # If running from within the notebooks directory, go up to project root\n",
    "    project_root = current_dir.parent\n",
    "elif 'data_preparation' in str(current_dir):\n",
    "    # If running from within the scripts directory, go up to project root\n",
    "    project_root = current_dir.parent.parent\n",
    "else:\n",
    "    # If running from project root\n",
    "    project_root = current_dir\n",
    "\n",
    "RAW_DATA_DIR = project_root / \"01_raw_data\"\n",
    "PROCESSED_DATA_DIR = project_root / \"02_processed_data\"\n",
    "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "UNICEF_FILE = RAW_DATA_DIR / \"GLOBAL_DATAFLOW_2018-2022.xlsx\"\n",
    "POPULATION_FILE = RAW_DATA_DIR / \"WPP2022_GEN_F01_DEMOGRAPHIC_INDICATORS_COMPACT_REV1.xlsx\"\n",
    "MORTALITY_FILE = RAW_DATA_DIR / \"On-track and off-track countries.xlsx\"\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Raw data directory: {RAW_DATA_DIR}\")\n",
    "print(f\"UNICEF file exists: {UNICEF_FILE.exists()}\")\n",
    "print(f\"Population file exists: {POPULATION_FILE.exists()}\")\n",
    "print(f\"Mortality file exists: {MORTALITY_FILE.exists()}\")\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Clean UNICEF Data\n",
    "\n",
    "Requirements:\n",
    "- Filter for years 2018-2022\n",
    "- Extract ANC4 and SBA indicators\n",
    "- Keep most recent estimate for each country-indicator\n",
    "- Convert to wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unicef_data():\n",
    "    \"\"\"\n",
    "    Clean UNICEF data:\n",
    "    - Filter for years 2018-2022\n",
    "    - Extract ANC4 and SBA indicators\n",
    "    - Keep most recent estimate for each country-indicator\n",
    "    - Convert to wide format\n",
    "    \"\"\"\n",
    "    print(\"Cleaning UNICEF data...\")\n",
    "    \n",
    "    # Read UNICEF data\n",
    "    df = pd.read_excel(UNICEF_FILE, sheet_name='Unicef data')\n",
    "    \n",
    "    # Filter for target years (2018-2022)\n",
    "    df = df[df['TIME_PERIOD'].between(2018, 2022)]\n",
    "    \n",
    "    # Identify ANC4 and SBA indicators based on exploration findings\n",
    "    anc4_mask = df['Indicator'].str.contains('Antenatal care 4+', na=False)\n",
    "    sba_mask = df['Indicator'].str.contains('Skilled birth attendant', na=False)\n",
    "    \n",
    "    # Filter for these indicators only\n",
    "    df = df[anc4_mask | sba_mask]\n",
    "    \n",
    "    # Create simplified indicator names\n",
    "    df['Indicator_Clean'] = df['Indicator'].apply(lambda x: \n",
    "        'ANC4' if 'Antenatal care 4+' in str(x) else 'SBA')\n",
    "    \n",
    "    # For each country-indicator combination, keep the most recent estimate\n",
    "    df_recent = df.sort_values('TIME_PERIOD').groupby(['Geographic area', 'Indicator_Clean']).tail(1)\n",
    "    \n",
    "    # Convert to wide format with ANC4 and SBA as separate columns\n",
    "    df_wide = df_recent.pivot_table(\n",
    "        index='Geographic area',\n",
    "        columns='Indicator_Clean',\n",
    "        values='OBS_VALUE',\n",
    "        aggfunc='first'\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    df_wide.columns.name = None\n",
    "    df_wide = df_wide.rename(columns={'Geographic area': 'Country'})\n",
    "    \n",
    "    print(f\"UNICEF data cleaned: {len(df_wide)} countries with health indicators\")\n",
    "    print(f\"Countries with ANC4 data: {df_wide['ANC4'].notna().sum()}\")\n",
    "    print(f\"Countries with SBA data: {df_wide['SBA'].notna().sum()}\")\n",
    "    \n",
    "    return df_wide\n",
    "\n",
    "# Execute cleaning\n",
    "unicef_data = clean_unicef_data()\n",
    "display(unicef_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Clean Population Data\n",
    "\n",
    "Requirements:\n",
    "- Extract 2022 birth projections\n",
    "- Convert to numeric format\n",
    "- Handle missing values appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_population_data():\n",
    "    \"\"\"\n",
    "    Clean population data:\n",
    "    - Extract 2022 birth projections\n",
    "    - Convert to numeric format\n",
    "    - Handle missing values\n",
    "    \"\"\"\n",
    "    print(\"\\nCleaning population data...\")\n",
    "    \n",
    "    # Read population data with proper header handling\n",
    "    # Based on exploration, header is at row 16\n",
    "    df_estimates = pd.read_excel(POPULATION_FILE, sheet_name='Estimates', header=16)\n",
    "    df_projections = pd.read_excel(POPULATION_FILE, sheet_name='Projections', header=16)\n",
    "    \n",
    "    # Use projections sheet for 2022 data as it's more recent\n",
    "    df = df_projections.copy()\n",
    "    \n",
    "    # Find the country column (based on exploration findings)\n",
    "    country_col = None\n",
    "    for col in df.columns:\n",
    "        if any(term in str(col).lower() for term in ['country', 'region', 'area']):\n",
    "            country_col = col\n",
    "            break\n",
    "    \n",
    "    if country_col is None:\n",
    "        # Fallback to first column if no clear country column found\n",
    "        country_col = df.columns[0]\n",
    "    \n",
    "    print(f\"Using country column: {country_col}\")\n",
    "    \n",
    "    # Filter for 2022 data\n",
    "    df_2022 = df[df['Year'] == 2022].copy()\n",
    "    \n",
    "    # Extract births data (look for births column)\n",
    "    births_col = None\n",
    "    for col in df.columns:\n",
    "        if 'births' in str(col).lower():\n",
    "            births_col = col\n",
    "            break\n",
    "    \n",
    "    if births_col is None:\n",
    "        # Look for columns that might contain birth data\n",
    "        for col in df.columns:\n",
    "            if any(term in str(col).lower() for term in ['birth', 'natality']):\n",
    "                births_col = col\n",
    "                break\n",
    "    \n",
    "    print(f\"Using births column: {births_col}\")\n",
    "    \n",
    "    if births_col is not None:\n",
    "        # Select relevant columns\n",
    "        pop_data = df_2022[[country_col, births_col]].copy()\n",
    "        pop_data.columns = ['Country', 'Births_2022']\n",
    "        \n",
    "        # Convert to numeric format\n",
    "        pop_data['Births_2022'] = pd.to_numeric(pop_data['Births_2022'], errors='coerce')\n",
    "        \n",
    "        # Handle missing values - remove rows with missing birth data\n",
    "        pop_data = pop_data.dropna(subset=['Births_2022'])\n",
    "        \n",
    "        print(f\"Population data cleaned: {len(pop_data)} countries with 2022 birth data\")\n",
    "    else:\n",
    "        print(\"Warning: Could not find births column in population data\")\n",
    "        # Create dummy data structure\n",
    "        pop_data = pd.DataFrame(columns=['Country', 'Births_2022'])\n",
    "    \n",
    "    return pop_data\n",
    "\n",
    "# Execute cleaning\n",
    "population_data = clean_population_data()\n",
    "display(population_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clean Mortality Classification\n",
    "\n",
    "Requirements:\n",
    "- Create binary classification: 'on-track' vs 'off-track'\n",
    "- On-track: status = \"achieved\" or \"on-track\"\n",
    "- Off-track: status = \"acceleration needed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_mortality_classification():\n",
    "    \"\"\"\n",
    "    Clean mortality classification:\n",
    "    - Create binary classification: 'on-track' vs 'off-track'\n",
    "    \"\"\"\n",
    "    print(\"\\nCleaning mortality classification...\")\n",
    "    \n",
    "    # Read mortality data\n",
    "    df = pd.read_excel(MORTALITY_FILE, sheet_name='Sheet1')\n",
    "    \n",
    "    # Create binary classification\n",
    "    def classify_status(status):\n",
    "        if pd.isna(status):\n",
    "            return np.nan\n",
    "        status_lower = str(status).lower()\n",
    "        if status_lower in ['achieved', 'on-track']:\n",
    "            return 'on-track'\n",
    "        elif 'acceleration needed' in status_lower:\n",
    "            return 'off-track'\n",
    "        else:\n",
    "            return 'off-track'  # Default to off-track for unknown statuses\n",
    "    \n",
    "    df['Mortality_Status_Binary'] = df['Status.U5MR'].apply(classify_status)\n",
    "    \n",
    "    # Select relevant columns\n",
    "    mortality_data = df[['OfficialName', 'ISO3Code', 'Mortality_Status_Binary']].copy()\n",
    "    mortality_data = mortality_data.rename(columns={'OfficialName': 'Country'})\n",
    "    \n",
    "    print(f\"Mortality classification cleaned: {len(mortality_data)} countries\")\n",
    "    print(f\"On-track countries: {(mortality_data['Mortality_Status_Binary'] == 'on-track').sum()}\")\n",
    "    print(f\"Off-track countries: {(mortality_data['Mortality_Status_Binary'] == 'off-track').sum()}\")\n",
    "    \n",
    "    return mortality_data\n",
    "\n",
    "# Execute cleaning\n",
    "mortality_data = clean_mortality_classification()\n",
    "display(mortality_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Standardize Country Names\n",
    "\n",
    "Create mapping for common country name variations and apply consistent naming across all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_country_mapping():\n",
    "    \"\"\"\n",
    "    Create mapping for common country name variations to standardize names across datasets.\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating country name mapping...\")\n",
    "    \n",
    "    # Common country name variations mapping\n",
    "    country_mapping = {\n",
    "        # Common variations\n",
    "        'United States of America': 'United States',\n",
    "        'USA': 'United States',\n",
    "        'US': 'United States',\n",
    "        'United Kingdom': 'United Kingdom of Great Britain and Northern Ireland',\n",
    "        'UK': 'United Kingdom of Great Britain and Northern Ireland',\n",
    "        'Russia': 'Russian Federation',\n",
    "        'South Korea': 'Republic of Korea',\n",
    "        'North Korea': \"Democratic People's Republic of Korea\",\n",
    "        'Iran': 'Iran (Islamic Republic of)',\n",
    "        'Venezuela': 'Venezuela (Bolivarian Republic of)',\n",
    "        'Bolivia': 'Bolivia (Plurinational State of)',\n",
    "        'Tanzania': 'United Republic of Tanzania',\n",
    "        'Congo': 'Congo',\n",
    "        'Democratic Republic of the Congo': 'Democratic Republic of the Congo',\n",
    "        'Ivory Coast': \"Côte d'Ivoire\",\n",
    "        'Cape Verde': 'Cabo Verde',\n",
    "        'Swaziland': 'Eswatini',\n",
    "        'Macedonia': 'North Macedonia',\n",
    "        'Myanmar': 'Myanmar',\n",
    "        'Burma': 'Myanmar',\n",
    "        'East Timor': 'Timor-Leste',\n",
    "        'Moldova': 'Republic of Moldova',\n",
    "        'Syria': 'Syrian Arab Republic',\n",
    "        'Laos': \"Lao People's Democratic Republic\",\n",
    "        'Vietnam': 'Viet Nam',\n",
    "        'Brunei': 'Brunei Darussalam',\n",
    "        'Micronesia': 'Micronesia (Federated States of)',\n",
    "        'Palestine': 'State of Palestine',\n",
    "        'Turkey': 'Türkiye'\n",
    "    }\n",
    "    \n",
    "    return country_mapping\n",
    "\n",
    "def standardize_country_names(df, country_col='Country', mapping=None):\n",
    "    \"\"\"\n",
    "    Apply country name standardization to a dataframe.\n",
    "    \"\"\"\n",
    "    if mapping is None:\n",
    "        mapping = create_country_mapping()\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[country_col] = df[country_col].replace(mapping)\n",
    "    \n",
    "    # Additional cleaning: strip whitespace and handle encoding issues\n",
    "    df[country_col] = df[country_col].astype(str).str.strip()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def filter_individual_countries(df, country_col='Country'):\n",
    "    \"\"\"\n",
    "    Filter out regional aggregates and keep only individual countries.\n",
    "    \"\"\"\n",
    "    # Patterns that indicate regional aggregates\n",
    "    regional_patterns = [\n",
    "        r'\\(.*SDGRC.*\\)',  # SDG regional classifications\n",
    "        r'Africa$',\n",
    "        r'Asia$',\n",
    "        r'Europe$',\n",
    "        r'America',\n",
    "        r'World',\n",
    "        r'Developed',\n",
    "        r'Developing',\n",
    "        r'Least developed',\n",
    "        r'Land.locked',\n",
    "        r'Small island',\n",
    "        r'Sub-Saharan',\n",
    "        r'Northern Africa',\n",
    "        r'Eastern Africa',\n",
    "        r'Western Africa',\n",
    "        r'Middle Africa',\n",
    "        r'Southern Africa',\n",
    "        r'Eastern Asia',\n",
    "        r'South-eastern Asia',\n",
    "        r'Southern Asia',\n",
    "        r'Western Asia',\n",
    "        r'Central Asia',\n",
    "        r'Eastern Europe',\n",
    "        r'Northern Europe',\n",
    "        r'Southern Europe',\n",
    "        r'Western Europe',\n",
    "        r'Caribbean',\n",
    "        r'Central America',\n",
    "        r'South America',\n",
    "        r'Northern America',\n",
    "        r'Oceania',\n",
    "        r'Polynesia',\n",
    "        r'Melanesia',\n",
    "        r'Micronesia',\n",
    "        r'More developed',\n",
    "        r'Less developed',\n",
    "        r'High income',\n",
    "        r'Upper middle income',\n",
    "        r'Lower middle income',\n",
    "        r'Low income'\n",
    "    ]\n",
    "    \n",
    "    # Create combined pattern\n",
    "    combined_pattern = '|'.join(regional_patterns)\n",
    "    \n",
    "    # Filter out regional aggregates\n",
    "    mask = ~df[country_col].str.contains(combined_pattern, case=False, na=False)\n",
    "    df_filtered = df[mask].copy()\n",
    "    \n",
    "    print(f\"Filtered from {len(df)} to {len(df_filtered)} individual countries\")\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "# Create country mapping\n",
    "country_mapping = create_country_mapping()\n",
    "print(f\"Created mapping for {len(country_mapping)} country name variations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Merge Datasets\n",
    "\n",
    "Use inner joins to keep only countries with complete data and validate merge results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(unicef_data, population_data, mortality_data):\n",
    "    \"\"\"\n",
    "    Merge datasets using inner joins to keep only countries with complete data.\n",
    "    \"\"\"\n",
    "    print(\"\\nMerging datasets...\")\n",
    "    \n",
    "    # Apply country name standardization to all datasets\n",
    "    country_mapping = create_country_mapping()\n",
    "    \n",
    "    unicef_clean = standardize_country_names(unicef_data, mapping=country_mapping)\n",
    "    population_clean = standardize_country_names(population_data, mapping=country_mapping)\n",
    "    mortality_clean = standardize_country_names(mortality_data, mapping=country_mapping)\n",
    "    \n",
    "    # Filter to individual countries only\n",
    "    unicef_clean = filter_individual_countries(unicef_clean)\n",
    "    population_clean = filter_individual_countries(population_clean)\n",
    "    mortality_clean = filter_individual_countries(mortality_clean)\n",
    "    \n",
    "    print(f\"After filtering:\")\n",
    "    print(f\"  UNICEF: {len(unicef_clean)} countries\")\n",
    "    print(f\"  Population: {len(population_clean)} countries\")\n",
    "    print(f\"  Mortality: {len(mortality_clean)} countries\")\n",
    "    \n",
    "    # Start with mortality data as it has the most standardized country names\n",
    "    merged = mortality_clean.copy()\n",
    "    \n",
    "    # Merge with UNICEF data\n",
    "    merged = merged.merge(unicef_clean, on='Country', how='inner')\n",
    "    print(f\"After merging with UNICEF data: {len(merged)} countries\")\n",
    "    \n",
    "    # Merge with population data\n",
    "    if len(population_clean) > 0:\n",
    "        merged = merged.merge(population_clean, on='Country', how='inner')\n",
    "        print(f\"After merging with population data: {len(merged)} countries\")\n",
    "    else:\n",
    "        print(\"Warning: No population data to merge\")\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Execute merge\n",
    "merged_data = merge_datasets(unicef_data, population_data, mortality_data)\n",
    "display(merged_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Validate Merge Results\n",
    "\n",
    "Validate the merge results and provide summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_merge_results(merged_data):\n",
    "    \"\"\"\n",
    "    Validate the merge results and provide summary statistics.\n",
    "    \"\"\"\n",
    "    print(\"\\nValidating merge results...\")\n",
    "    print(f\"Final dataset shape: {merged_data.shape}\")\n",
    "    print(f\"Columns: {list(merged_data.columns)}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values:\")\n",
    "    missing_counts = merged_data.isnull().sum()\n",
    "    for col, count in missing_counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count} ({count/len(merged_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Summary statistics for numeric columns\n",
    "    numeric_cols = merged_data.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(\"\\nSummary statistics for numeric columns:\")\n",
    "        display(merged_data[numeric_cols].describe())\n",
    "    \n",
    "    # Check mortality status distribution\n",
    "    if 'Mortality_Status_Binary' in merged_data.columns:\n",
    "        print(\"\\nMortality status distribution:\")\n",
    "        status_counts = merged_data['Mortality_Status_Binary'].value_counts()\n",
    "        for status, count in status_counts.items():\n",
    "            print(f\"  {status}: {count} countries ({count/len(merged_data)*100:.1f}%)\")\n",
    "    \n",
    "    # Check health indicator availability\n",
    "    if 'ANC4' in merged_data.columns:\n",
    "        anc4_available = merged_data['ANC4'].notna().sum()\n",
    "        print(f\"\\nCountries with ANC4 data: {anc4_available} ({anc4_available/len(merged_data)*100:.1f}%)\")\n",
    "    \n",
    "    if 'SBA' in merged_data.columns:\n",
    "        sba_available = merged_data['SBA'].notna().sum()\n",
    "        print(f\"Countries with SBA data: {sba_available} ({sba_available/len(merged_data)*100:.1f}%)\")\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "# Execute validation\n",
    "validated_data = validate_merge_results(merged_data)\n",
    "display(validated_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Final Dataset\n",
    "\n",
    "Save the final merged dataset as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_final_dataset(merged_data, filename=\"merged_health_data.csv\"):\n",
    "    \"\"\"\n",
    "    Save the final merged dataset as CSV.\n",
    "    \"\"\"\n",
    "    output_path = PROCESSED_DATA_DIR / filename\n",
    "    merged_data.to_csv(output_path, index=False)\n",
    "    print(f\"\\nFinal dataset saved to: {output_path}\")\n",
    "    print(f\"Dataset contains {len(merged_data)} countries with complete data\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Save final dataset\n",
    "output_path = save_final_dataset(validated_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA CLEANING AND MERGING COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Final dataset: {output_path}\")\n",
    "print(f\"Countries included: {len(validated_data)}\")\n",
    "print(f\"Variables: {list(validated_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Dataset Preview\n",
    "\n",
    "Display the final cleaned and merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final dataset\n",
    "print(\"Final Dataset Preview:\")\n",
    "display(validated_data)\n",
    "\n",
    "# Show sample of countries by mortality status\n",
    "if 'Mortality_Status_Binary' in validated_data.columns:\n",
    "    print(\"\\nSample countries by mortality status:\")\n",
    "    for status in validated_data['Mortality_Status_Binary'].unique():\n",
    "        if pd.notna(status):\n",
    "            sample_countries = validated_data[validated_data['Mortality_Status_Binary'] == status]['Country'].head(5).tolist()\n",
    "            print(f\"  {status}: {', '.join(sample_countries)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "litefarm_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
